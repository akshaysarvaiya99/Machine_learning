{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW-Method using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-01 14:17:31,467 : INFO : collecting all words and their counts\n",
      "2018-12-01 14:18:25,393 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-12-01 14:18:26,611 : INFO : PROGRESS: at sentence #10000, processed 500000 words, keeping 33464 word types\n",
      "2018-12-01 14:18:26,740 : INFO : PROGRESS: at sentence #20000, processed 1000000 words, keeping 52755 word types\n",
      "2018-12-01 14:18:26,869 : INFO : PROGRESS: at sentence #30000, processed 1500000 words, keeping 65589 word types\n",
      "2018-12-01 14:18:27,000 : INFO : PROGRESS: at sentence #40000, processed 2000000 words, keeping 78383 word types\n",
      "2018-12-01 14:18:27,133 : INFO : PROGRESS: at sentence #50000, processed 2500000 words, keeping 88008 word types\n",
      "2018-12-01 14:18:27,264 : INFO : PROGRESS: at sentence #60000, processed 3000000 words, keeping 96645 word types\n",
      "2018-12-01 14:18:27,395 : INFO : PROGRESS: at sentence #70000, processed 3500000 words, keeping 104309 word types\n",
      "2018-12-01 14:18:27,526 : INFO : PROGRESS: at sentence #80000, processed 4000000 words, keeping 111461 word types\n",
      "2018-12-01 14:18:27,658 : INFO : PROGRESS: at sentence #90000, processed 4500000 words, keeping 118752 word types\n",
      "2018-12-01 14:18:27,793 : INFO : PROGRESS: at sentence #100000, processed 5000000 words, keeping 125355 word types\n",
      "2018-12-01 14:18:27,927 : INFO : PROGRESS: at sentence #110000, processed 5500000 words, keeping 133141 word types\n",
      "2018-12-01 14:18:28,059 : INFO : PROGRESS: at sentence #120000, processed 6000000 words, keeping 139566 word types\n",
      "2018-12-01 14:18:28,191 : INFO : PROGRESS: at sentence #130000, processed 6500000 words, keeping 145782 word types\n",
      "2018-12-01 14:18:28,331 : INFO : PROGRESS: at sentence #140000, processed 7000000 words, keeping 151934 word types\n",
      "2018-12-01 14:18:28,467 : INFO : PROGRESS: at sentence #150000, processed 7500000 words, keeping 158046 word types\n",
      "2018-12-01 14:18:28,601 : INFO : PROGRESS: at sentence #160000, processed 8000000 words, keeping 164115 word types\n",
      "2018-12-01 14:18:28,840 : INFO : PROGRESS: at sentence #170000, processed 8500000 words, keeping 171256 word types\n",
      "2018-12-01 14:18:29,136 : INFO : PROGRESS: at sentence #180000, processed 9000000 words, keeping 178163 word types\n",
      "2018-12-01 14:18:29,414 : INFO : PROGRESS: at sentence #190000, processed 9500000 words, keeping 184129 word types\n",
      "2018-12-01 14:18:29,694 : INFO : PROGRESS: at sentence #200000, processed 10000000 words, keeping 189075 word types\n",
      "2018-12-01 14:18:29,973 : INFO : PROGRESS: at sentence #210000, processed 10500000 words, keeping 194511 word types\n",
      "2018-12-01 14:18:30,257 : INFO : PROGRESS: at sentence #220000, processed 11000000 words, keeping 198758 word types\n",
      "2018-12-01 14:18:30,546 : INFO : PROGRESS: at sentence #230000, processed 11500000 words, keeping 203441 word types\n",
      "2018-12-01 14:18:30,824 : INFO : PROGRESS: at sentence #240000, processed 12000000 words, keeping 207895 word types\n",
      "2018-12-01 14:18:31,103 : INFO : PROGRESS: at sentence #250000, processed 12500000 words, keeping 212668 word types\n",
      "2018-12-01 14:18:31,380 : INFO : PROGRESS: at sentence #260000, processed 13000000 words, keeping 217128 word types\n",
      "2018-12-01 14:18:31,658 : INFO : PROGRESS: at sentence #270000, processed 13500000 words, keeping 221416 word types\n",
      "2018-12-01 14:18:31,863 : INFO : PROGRESS: at sentence #280000, processed 14000000 words, keeping 226855 word types\n",
      "2018-12-01 14:18:31,998 : INFO : PROGRESS: at sentence #290000, processed 14500000 words, keeping 231424 word types\n",
      "2018-12-01 14:18:32,133 : INFO : PROGRESS: at sentence #300000, processed 15000000 words, keeping 237391 word types\n",
      "2018-12-01 14:18:32,268 : INFO : PROGRESS: at sentence #310000, processed 15500000 words, keeping 241697 word types\n",
      "2018-12-01 14:18:32,423 : INFO : PROGRESS: at sentence #320000, processed 16000000 words, keeping 245649 word types\n",
      "2018-12-01 14:18:33,733 : INFO : PROGRESS: at sentence #330000, processed 16500000 words, keeping 249621 word types\n",
      "2018-12-01 14:18:34,461 : INFO : PROGRESS: at sentence #340000, processed 17000000 words, keeping 253834 word types\n",
      "2018-12-01 14:18:34,761 : INFO : collected 253855 word types from a corpus of 17005208 raw words and 340105 sentences\n",
      "2018-12-01 14:18:34,762 : INFO : Loading a fresh vocabulary\n",
      "2018-12-01 14:18:37,204 : INFO : min_count=30 retains 25097 unique words (9% of original 253855, drops 228758)\n",
      "2018-12-01 14:18:37,205 : INFO : min_count=30 leaves 16191060 word corpus (95% of original 17005208, drops 814148)\n",
      "2018-12-01 14:18:37,545 : INFO : deleting the raw counts dictionary of 253855 items\n",
      "2018-12-01 14:18:37,623 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2018-12-01 14:18:37,623 : INFO : downsampling leaves estimated 11928484 word corpus (73.7% of prior 16191060)\n",
      "2018-12-01 14:18:37,722 : INFO : estimated required memory for 25097 words and 300 dimensions: 72781300 bytes\n",
      "2018-12-01 14:18:37,722 : INFO : resetting layer weights\n",
      "2018-12-01 14:18:38,176 : INFO : training model with 3 workers on 25097 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-12-01 14:18:52,493 : INFO : EPOCH 1 - PROGRESS: at 0.06% examples, 506 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:18:53,490 : INFO : EPOCH 1 - PROGRESS: at 1.23% examples, 9979 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:18:54,494 : INFO : EPOCH 1 - PROGRESS: at 7.47% examples, 55031 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:18:55,513 : INFO : EPOCH 1 - PROGRESS: at 12.11% examples, 84006 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:18:56,520 : INFO : EPOCH 1 - PROGRESS: at 15.58% examples, 102203 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:18:57,538 : INFO : EPOCH 1 - PROGRESS: at 19.17% examples, 119251 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:18:58,540 : INFO : EPOCH 1 - PROGRESS: at 22.76% examples, 134686 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:18:59,544 : INFO : EPOCH 1 - PROGRESS: at 26.23% examples, 148180 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:00,554 : INFO : EPOCH 1 - PROGRESS: at 29.76% examples, 160756 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:01,567 : INFO : EPOCH 1 - PROGRESS: at 33.28% examples, 172091 words/s, in_qsize 4, out_qsize 1\n",
      "2018-12-01 14:19:02,582 : INFO : EPOCH 1 - PROGRESS: at 36.93% examples, 183104 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:03,592 : INFO : EPOCH 1 - PROGRESS: at 40.58% examples, 192998 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:04,602 : INFO : EPOCH 1 - PROGRESS: at 44.05% examples, 201459 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:05,612 : INFO : EPOCH 1 - PROGRESS: at 47.51% examples, 209318 words/s, in_qsize 5, out_qsize 1\n",
      "2018-12-01 14:19:06,631 : INFO : EPOCH 1 - PROGRESS: at 50.98% examples, 216531 words/s, in_qsize 6, out_qsize 1\n",
      "2018-12-01 14:19:07,647 : INFO : EPOCH 1 - PROGRESS: at 54.51% examples, 223442 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:08,652 : INFO : EPOCH 1 - PROGRESS: at 58.04% examples, 229968 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:09,655 : INFO : EPOCH 1 - PROGRESS: at 61.45% examples, 235668 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:10,661 : INFO : EPOCH 1 - PROGRESS: at 65.10% examples, 241831 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:11,661 : INFO : EPOCH 1 - PROGRESS: at 68.57% examples, 247050 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:12,663 : INFO : EPOCH 1 - PROGRESS: at 72.04% examples, 251999 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:13,671 : INFO : EPOCH 1 - PROGRESS: at 75.68% examples, 256952 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:14,691 : INFO : EPOCH 1 - PROGRESS: at 79.39% examples, 261671 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:15,711 : INFO : EPOCH 1 - PROGRESS: at 82.97% examples, 265875 words/s, in_qsize 5, out_qsize 1\n",
      "2018-12-01 14:19:16,713 : INFO : EPOCH 1 - PROGRESS: at 86.62% examples, 270340 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:17,719 : INFO : EPOCH 1 - PROGRESS: at 90.09% examples, 273980 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:18,721 : INFO : EPOCH 1 - PROGRESS: at 93.56% examples, 277344 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:19,737 : INFO : EPOCH 1 - PROGRESS: at 97.03% examples, 280482 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-01 14:19:20,799 : INFO : EPOCH 1 - PROGRESS: at 99.56% examples, 280543 words/s, in_qsize 6, out_qsize 1\n",
      "2018-12-01 14:19:20,904 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 14:19:20,911 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 14:19:20,917 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 14:19:20,919 : INFO : EPOCH - 1 : training on 17005208 raw words (11929472 effective words) took 42.5s, 281013 effective words/s\n",
      "2018-12-01 14:19:24,815 : INFO : EPOCH 2 - PROGRESS: at 0.06% examples, 1960 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-01 14:19:25,821 : INFO : EPOCH 2 - PROGRESS: at 5.88% examples, 144665 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:26,843 : INFO : EPOCH 2 - PROGRESS: at 11.00% examples, 221885 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:27,851 : INFO : EPOCH 2 - PROGRESS: at 14.64% examples, 252640 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:28,852 : INFO : EPOCH 2 - PROGRESS: at 18.17% examples, 274347 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:29,859 : INFO : EPOCH 2 - PROGRESS: at 21.70% examples, 290597 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:30,860 : INFO : EPOCH 2 - PROGRESS: at 25.29% examples, 304993 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:31,862 : INFO : EPOCH 2 - PROGRESS: at 28.81% examples, 316225 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:32,869 : INFO : EPOCH 2 - PROGRESS: at 32.40% examples, 325779 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:33,890 : INFO : EPOCH 2 - PROGRESS: at 35.99% examples, 333640 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:34,914 : INFO : EPOCH 2 - PROGRESS: at 39.58% examples, 339898 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:35,915 : INFO : EPOCH 2 - PROGRESS: at 43.22% examples, 346401 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:36,937 : INFO : EPOCH 2 - PROGRESS: at 46.69% examples, 350262 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:37,941 : INFO : EPOCH 2 - PROGRESS: at 50.16% examples, 353997 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:38,963 : INFO : EPOCH 2 - PROGRESS: at 53.81% examples, 358088 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:39,987 : INFO : EPOCH 2 - PROGRESS: at 57.51% examples, 362190 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:40,989 : INFO : EPOCH 2 - PROGRESS: at 60.92% examples, 364475 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:42,000 : INFO : EPOCH 2 - PROGRESS: at 64.45% examples, 366988 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:43,020 : INFO : EPOCH 2 - PROGRESS: at 68.10% examples, 369804 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:44,023 : INFO : EPOCH 2 - PROGRESS: at 71.68% examples, 372409 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:45,031 : INFO : EPOCH 2 - PROGRESS: at 75.21% examples, 374191 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:46,048 : INFO : EPOCH 2 - PROGRESS: at 78.86% examples, 375872 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:47,070 : INFO : EPOCH 2 - PROGRESS: at 82.45% examples, 377378 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:19:48,077 : INFO : EPOCH 2 - PROGRESS: at 86.03% examples, 379222 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:49,077 : INFO : EPOCH 2 - PROGRESS: at 89.56% examples, 380672 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:50,092 : INFO : EPOCH 2 - PROGRESS: at 93.15% examples, 382015 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:51,097 : INFO : EPOCH 2 - PROGRESS: at 96.73% examples, 383406 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:52,203 : INFO : EPOCH 2 - PROGRESS: at 99.44% examples, 380060 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:52,311 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 14:19:52,333 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 14:19:52,356 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 14:19:52,357 : INFO : EPOCH - 2 : training on 17005208 raw words (11928253 effective words) took 31.4s, 380357 effective words/s\n",
      "2018-12-01 14:19:56,095 : INFO : EPOCH 3 - PROGRESS: at 0.06% examples, 1999 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-01 14:19:57,096 : INFO : EPOCH 3 - PROGRESS: at 5.00% examples, 125807 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:58,112 : INFO : EPOCH 3 - PROGRESS: at 8.70% examples, 178787 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:19:59,123 : INFO : EPOCH 3 - PROGRESS: at 12.29% examples, 214634 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:00,133 : INFO : EPOCH 3 - PROGRESS: at 15.82% examples, 240820 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:01,144 : INFO : EPOCH 3 - PROGRESS: at 19.41% examples, 261925 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:02,171 : INFO : EPOCH 3 - PROGRESS: at 22.99% examples, 278374 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:03,193 : INFO : EPOCH 3 - PROGRESS: at 26.52% examples, 291394 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:04,205 : INFO : EPOCH 3 - PROGRESS: at 30.05% examples, 302599 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:05,208 : INFO : EPOCH 3 - PROGRESS: at 33.64% examples, 312585 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:06,219 : INFO : EPOCH 3 - PROGRESS: at 37.28% examples, 321473 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:07,225 : INFO : EPOCH 3 - PROGRESS: at 40.81% examples, 328052 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:08,237 : INFO : EPOCH 3 - PROGRESS: at 44.46% examples, 334725 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:09,248 : INFO : EPOCH 3 - PROGRESS: at 47.93% examples, 339327 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:10,257 : INFO : EPOCH 3 - PROGRESS: at 51.45% examples, 343803 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:11,263 : INFO : EPOCH 3 - PROGRESS: at 54.92% examples, 347464 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:12,272 : INFO : EPOCH 3 - PROGRESS: at 58.39% examples, 350726 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:13,275 : INFO : EPOCH 3 - PROGRESS: at 61.80% examples, 353408 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:14,276 : INFO : EPOCH 3 - PROGRESS: at 65.27% examples, 356205 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:15,294 : INFO : EPOCH 3 - PROGRESS: at 68.80% examples, 358779 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:16,296 : INFO : EPOCH 3 - PROGRESS: at 72.33% examples, 361488 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:17,303 : INFO : EPOCH 3 - PROGRESS: at 75.92% examples, 363718 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:18,326 : INFO : EPOCH 3 - PROGRESS: at 79.50% examples, 365583 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:19,333 : INFO : EPOCH 3 - PROGRESS: at 83.09% examples, 367630 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:20,335 : INFO : EPOCH 3 - PROGRESS: at 86.56% examples, 369319 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:21,340 : INFO : EPOCH 3 - PROGRESS: at 90.21% examples, 371564 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:22,370 : INFO : EPOCH 3 - PROGRESS: at 93.85% examples, 373115 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:23,383 : INFO : EPOCH 3 - PROGRESS: at 97.38% examples, 374448 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:24,386 : INFO : EPOCH 3 - PROGRESS: at 99.62% examples, 371004 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:24,449 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 14:20:24,471 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 14:20:24,489 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 14:20:24,491 : INFO : EPOCH - 3 : training on 17005208 raw words (11928025 effective words) took 32.1s, 371253 effective words/s\n",
      "2018-12-01 14:20:26,536 : INFO : EPOCH 4 - PROGRESS: at 0.06% examples, 3654 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:27,538 : INFO : EPOCH 4 - PROGRESS: at 3.59% examples, 141167 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:28,554 : INFO : EPOCH 4 - PROGRESS: at 7.12% examples, 206655 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:29,556 : INFO : EPOCH 4 - PROGRESS: at 10.64% examples, 248304 words/s, in_qsize 5, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-01 14:20:30,560 : INFO : EPOCH 4 - PROGRESS: at 14.35% examples, 279776 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:31,564 : INFO : EPOCH 4 - PROGRESS: at 17.94% examples, 300835 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:32,569 : INFO : EPOCH 4 - PROGRESS: at 21.46% examples, 315418 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:33,583 : INFO : EPOCH 4 - PROGRESS: at 24.99% examples, 326970 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:34,592 : INFO : EPOCH 4 - PROGRESS: at 28.46% examples, 335973 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:35,608 : INFO : EPOCH 4 - PROGRESS: at 32.17% examples, 345404 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:36,622 : INFO : EPOCH 4 - PROGRESS: at 35.75% examples, 352219 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:37,641 : INFO : EPOCH 4 - PROGRESS: at 39.46% examples, 358642 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:38,649 : INFO : EPOCH 4 - PROGRESS: at 43.05% examples, 363516 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:39,659 : INFO : EPOCH 4 - PROGRESS: at 46.63% examples, 367614 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:40,666 : INFO : EPOCH 4 - PROGRESS: at 50.10% examples, 370434 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:41,681 : INFO : EPOCH 4 - PROGRESS: at 53.63% examples, 373038 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:42,686 : INFO : EPOCH 4 - PROGRESS: at 57.10% examples, 375370 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:43,692 : INFO : EPOCH 4 - PROGRESS: at 60.75% examples, 378416 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:44,694 : INFO : EPOCH 4 - PROGRESS: at 64.39% examples, 381199 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:45,708 : INFO : EPOCH 4 - PROGRESS: at 68.04% examples, 383510 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:46,727 : INFO : EPOCH 4 - PROGRESS: at 71.62% examples, 385303 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:47,728 : INFO : EPOCH 4 - PROGRESS: at 75.27% examples, 387296 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:48,734 : INFO : EPOCH 4 - PROGRESS: at 78.98% examples, 388953 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:49,744 : INFO : EPOCH 4 - PROGRESS: at 82.45% examples, 389620 words/s, in_qsize 5, out_qsize 1\n",
      "2018-12-01 14:20:50,760 : INFO : EPOCH 4 - PROGRESS: at 85.74% examples, 389565 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:51,770 : INFO : EPOCH 4 - PROGRESS: at 88.85% examples, 388682 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:52,791 : INFO : EPOCH 4 - PROGRESS: at 92.44% examples, 389727 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:53,809 : INFO : EPOCH 4 - PROGRESS: at 95.85% examples, 390068 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:54,813 : INFO : EPOCH 4 - PROGRESS: at 99.20% examples, 390171 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:20:55,329 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 14:20:55,337 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 14:20:55,349 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 14:20:55,350 : INFO : EPOCH - 4 : training on 17005208 raw words (11925831 effective words) took 30.9s, 386506 effective words/s\n",
      "2018-12-01 14:20:57,193 : INFO : EPOCH 5 - PROGRESS: at 0.06% examples, 4068 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-01 14:20:58,202 : INFO : EPOCH 5 - PROGRESS: at 3.59% examples, 150917 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:20:59,227 : INFO : EPOCH 5 - PROGRESS: at 7.29% examples, 222281 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:00,239 : INFO : EPOCH 5 - PROGRESS: at 10.82% examples, 261488 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:01,250 : INFO : EPOCH 5 - PROGRESS: at 14.35% examples, 287921 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:21:02,256 : INFO : EPOCH 5 - PROGRESS: at 17.99% examples, 309183 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:03,266 : INFO : EPOCH 5 - PROGRESS: at 21.52% examples, 322872 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:21:04,293 : INFO : EPOCH 5 - PROGRESS: at 25.11% examples, 334130 words/s, in_qsize 6, out_qsize 1\n",
      "2018-12-01 14:21:05,295 : INFO : EPOCH 5 - PROGRESS: at 28.70% examples, 344217 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:06,320 : INFO : EPOCH 5 - PROGRESS: at 32.40% examples, 352691 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:07,328 : INFO : EPOCH 5 - PROGRESS: at 35.99% examples, 359286 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:08,329 : INFO : EPOCH 5 - PROGRESS: at 39.52% examples, 364078 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:21:09,360 : INFO : EPOCH 5 - PROGRESS: at 43.05% examples, 367508 words/s, in_qsize 6, out_qsize 1\n",
      "2018-12-01 14:21:10,379 : INFO : EPOCH 5 - PROGRESS: at 46.75% examples, 372097 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:11,379 : INFO : EPOCH 5 - PROGRESS: at 50.28% examples, 375190 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:12,383 : INFO : EPOCH 5 - PROGRESS: at 53.92% examples, 378659 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:13,385 : INFO : EPOCH 5 - PROGRESS: at 57.51% examples, 381467 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:21:14,428 : INFO : EPOCH 5 - PROGRESS: at 61.16% examples, 383491 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:15,451 : INFO : EPOCH 5 - PROGRESS: at 64.69% examples, 384945 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:16,464 : INFO : EPOCH 5 - PROGRESS: at 68.21% examples, 386473 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:17,476 : INFO : EPOCH 5 - PROGRESS: at 71.80% examples, 388250 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:18,490 : INFO : EPOCH 5 - PROGRESS: at 75.33% examples, 389303 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:21:19,497 : INFO : EPOCH 5 - PROGRESS: at 78.80% examples, 389750 words/s, in_qsize 5, out_qsize 1\n",
      "2018-12-01 14:21:20,499 : INFO : EPOCH 5 - PROGRESS: at 82.33% examples, 390801 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:21:21,507 : INFO : EPOCH 5 - PROGRESS: at 85.91% examples, 392159 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:22,508 : INFO : EPOCH 5 - PROGRESS: at 89.56% examples, 393689 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:21:23,516 : INFO : EPOCH 5 - PROGRESS: at 93.15% examples, 394712 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:24,519 : INFO : EPOCH 5 - PROGRESS: at 96.73% examples, 395737 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-01 14:21:25,621 : INFO : EPOCH 5 - PROGRESS: at 99.56% examples, 392363 words/s, in_qsize 6, out_qsize 0\n",
      "2018-12-01 14:21:25,720 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 14:21:25,728 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 14:21:25,741 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 14:21:25,742 : INFO : EPOCH - 5 : training on 17005208 raw words (11929164 effective words) took 30.4s, 392566 effective words/s\n",
      "2018-12-01 14:21:25,743 : INFO : training on a 85026040 raw words (59640745 effective words) took 167.6s, 355923 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "class Text8Sentences(object):\n",
    "    def __init__(self,fname,maxlen):\n",
    "        self.fname = fname\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "    def __iter__(self,):\n",
    "           with open(self.fname, \"r\") as ftext:\n",
    "                text = ftext.read().split(\" \")\n",
    "                words = []\n",
    "                for word in text:\n",
    "                    if len(words) >= self.maxlen:\n",
    "                        yield words\n",
    "                        words = []\n",
    "                    words.append(word)\n",
    "                yield words\n",
    "\n",
    "\n",
    "data = \"./\"\n",
    "sentences = Text8Sentences(os.path.join(data, \"text8.txt\"), 50)\n",
    "model = word2vec.Word2Vec(sentences, size=300, min_count=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-01 14:22:18,970 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girl', 0.7142775654792786), ('child', 0.6974591016769409), ('man', 0.6733200550079346), ('herself', 0.6407644748687744), ('lady', 0.6313808560371399), ('baby', 0.630010187625885), ('person', 0.6156458854675293), ('lover', 0.6108782291412354), ('prostitute', 0.6080262660980225), ('mother', 0.5998956561088562)]\n"
     ]
    }
   ],
   "source": [
    "#most_similar\n",
    "\n",
    "print(model.most_similar(\"woman\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.6132511496543884), ('princess', 0.5554727911949158), ('empress', 0.5432103276252747), ('daughter', 0.5391123294830322), ('throne', 0.5376384854316711), ('isabella', 0.5371761322021484), ('prince', 0.5333335399627686), ('elizabeth', 0.5304224491119385), ('pharaoh', 0.5212265253067017), ('consort', 0.515372633934021)]\n"
     ]
    }
   ],
   "source": [
    "#most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"], topn=10\n",
    "\n",
    "print(model.most_similar(positive=['woman', 'king'], negative=['man'], topn=10))                     \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142774113026622\n",
      "0.5774182894329365\n",
      "0.2978809985081157\n",
      "0.48169954077933447\n"
     ]
    }
   ],
   "source": [
    "#similarity(\"girl\", \"woman\")\n",
    "\n",
    "print(model.similarity(\"girl\", \"woman\"))\n",
    "\n",
    "#similarity(\"girl\", \"man\")\n",
    "\n",
    "print(model.similarity(\"girl\", \"man\"))\n",
    "\n",
    "#similarity(\"girl\", \"car\")\n",
    "\n",
    "print(model.similarity(\"girl\", \"car\"))\n",
    "\n",
    "#similarity(\"bus\", \"car\")\n",
    "\n",
    "print(model.similarity(\"bus\", \"car\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgrams-Method using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(is (12), script (14)) -> 1\n",
      "(script (14), the (13)) -> 0\n",
      "(chart (7), js (8)) -> 1\n",
      "(started (5), easy (2)) -> 0\n",
      "(the (13), easy (2)) -> 0\n",
      "(all (9), chart (7)) -> 1\n",
      "(the (13), all (9)) -> 1\n",
      "(is (12), all (9)) -> 0\n",
      "(to (3), required (11)) -> 0\n",
      "(page (18), included (15)) -> 0\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "text = \"It's easy to get started with Chart.js. All that's required is the script included in your page.\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "wids = [word2id[w] for w in text_to_word_sequence(text)]\n",
    "pairs, labels = skipgrams(wids, len(word2id))\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(id2word[pairs[i][0]], pairs[i][0], id2word[pairs[i][1]], pairs[i][1],labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW model weight generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Lambda,Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import keras.backend as K\n",
    "import nltk\n",
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1579\n",
    "embed_size = 300\n",
    "window_size = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embed_size, embeddings_initializer='glorot_uniform', input_length=window_size*2))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "model.add(Dense(vocab_size, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adadelta\")\n",
    "\n",
    "# get weights\n",
    "weights = model.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opportunity => refused, ago, he’ll, ‘hold, along, thousand, falling, dodo, ‘mine, usual\n",
      "Catch => nearly, conversations, garden, expecting, ‘w, where, pointing, become, cunning, how\n"
     ]
    }
   ],
   "source": [
    "lines = []\n",
    "fin = open(\"./alice_in_wonderland.txt\", \"r\")\n",
    "for line in fin:\n",
    "    line = line.strip()\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    lines.append(line)\n",
    "fin.close()\n",
    "\n",
    "sents = nltk.sent_tokenize(\" \".join(lines))\n",
    "\n",
    "tokenizer = Tokenizer(20000)  # use top 5000 words only\n",
    "tokens = tokenizer.fit_on_texts(sents)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "w_lefts, w_centers, w_rights = [], [], []\n",
    "for sent in sents:\n",
    "    embedding = one_hot(sent, vocab_size)\n",
    "    triples = list(nltk.trigrams(embedding))\n",
    "    w_lefts.extend([x[0] for x in triples])\n",
    "    w_centers.extend([x[1] for x in triples])\n",
    "    w_rights.extend([x[2] for x in triples])\n",
    "\n",
    "ohe = OneHotEncoder(n_values=vocab_size)\n",
    "Xleft = ohe.fit_transform(np.array(w_lefts).reshape(-1, 1)).todense()\n",
    "Xright = ohe.fit_transform(np.array(w_rights).reshape(-1, 1)).todense()\n",
    "X = (Xleft + Xright) / 2.0\n",
    "Y = ohe.fit_transform(np.array(w_centers).reshape(-1, 1)).todense()\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "\n",
    "idx2emb = {}    \n",
    "for word in word2idx.keys():\n",
    "    wid = word2idx[word]\n",
    "    vec_in = ohe.fit_transform(np.array(wid)).todense()\n",
    "    vec_emb = np.dot(vec_in, weights)\n",
    "    idx2emb[wid] = vec_emb\n",
    "\n",
    "for word in [\"opportunity\", \"Catch\"]:\n",
    "    wid = word2idx[word.lower()]\n",
    "    source_emb = idx2emb[wid]\n",
    "    distances = []\n",
    "    for i in range(1, vocab_size):\n",
    "        if i == wid:\n",
    "            continue\n",
    "        target_emb = idx2emb[i]\n",
    "        distances.append(((wid, i), cosine_distances(source_emb, target_emb)))\n",
    "    sorted_distances = sorted(distances, key=operator.itemgetter(1))[0:10]\n",
    "    predictions = [idx2word[x[0][1]] for x in sorted_distances]\n",
    "    print(\"{:s} => {:s}\".format(word, \", \".join(predictions)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
